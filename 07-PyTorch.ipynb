{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is a machine learning framework based on [Torch](https://en.wikipedia.org/wiki/Torch_(machine_learning)), which is no longer developed. There are two high level concepts you need to be familiar with, Tensors and Auto-Grad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing PyTorch\n",
    "PyTorch is well documented and provides easy [installation instructions](https://pytorch.org/get-started/locally/). Generally you're going to be installing it to a conda environment, so the command below should be enough to get it on your system.\n",
    "\n",
    "```\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "\n",
    "If you struggle with installation, run into any errors or have any concerns don't hesistate to reach out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch\n",
    "It all starts with a simple import. From there we can explore the functionality PyTorch has to offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch # It all starts with a simple import...\n",
    "\n",
    "print(torch.__version__) # Version check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're ever having an issue with something not working the way it's described in the documentation it's very likely a version issue.\n",
    "\n",
    "## Tensors\n",
    "Up until this point in the semester we've been using numpy to preform our calculations, PyTorch pretty much works the same way. Instead of using arrays we use [Tensors](https://pytorch.org/docs/stable/tensors.html). In this case tensors refer to a multi-dimensional matrix containing a singular data type. \n",
    "\n",
    "Tensors can use a variety of data types. However they are not *Python* data types, they are instead *PyTorch* data types. This is important to recognize because you could be working with an 8 bit representation when you think you're working with a 64 bit representation! Below are a few of the types that PyTorch supports. If you have a GPU and install CUDA you will have access to even more types, but that's for another time. \n",
    "\n",
    "### Data Types\n",
    "| Data type               | dtype        | CPU Type           | Bits |\n",
    "|-------------------------|--------------|--------------------|------|\n",
    "| 32-bit floating point   | torch.float  | torch.FloatTensor  | 32   |\n",
    "| 64-bit floating point   | torch.double | torch.DoubleTensor | 64   |\n",
    "| 16-bit integer (signed) | torch.short  | torch.ShortTensor  | 16   |\n",
    "| 32-bit integer (signed) | torch.int    | torch.IntTensor    | 32   |\n",
    "| 8-bit integer (signed)  | torch.int8   | torch.CharTensor   | 8    |\n",
    "\n",
    "You should be familiar with a few of these types, even if you haven't touched them in years. By default tensors in PyTorch use a 32-bit floating point representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor()\n",
    "\n",
    "print(tensor.dtype) # The type is stored in the `dtype` field\n",
    "print(tensor) # Also it's completely empty :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a tensor lets add some data to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/python-intro/07-PyTorch.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B653a5c53534420447269766520446f63756d656e74735c5265706f735c707974686f6e2d696e74726f/workspaces/python-intro/07-PyTorch.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tensor[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "tensor[0] = 0 # Oh..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly we can't just add data to this tensor, there's no place to put any! Instead let's create a couple new tensors. Two using a Python list, another with a numpy array and a third that's just a bunch of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4], dtype=torch.int32)\n",
      "\n",
      "\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "================================================================\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]]\n",
      "\n",
      "\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]], dtype=torch.int16)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_to_tensor = torch.tensor([1, 2, 3, 4], dtype=torch.int) # A 1D list as an int tensor\n",
    "print(str(list_to_tensor) + \"\\n\" * 2)\n",
    "\n",
    "nested_list_to_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float) # A nested list as an int tensor\n",
    "print(str(nested_list_to_tensor) + \"\\n\" )\n",
    "\n",
    "print(\"=\" * 64)\n",
    "\n",
    "np_arr = np.array([[[1, 2], [3, 4]], [[5, 6] ,[7, 8]]], dtype=np.short) # Lets make a 3D matrix that's a short\n",
    "print(str(np_arr)  + \"\\n\" * 2)\n",
    "\n",
    "arr_to_tensor = torch.tensor(np_arr) # Pass it through...\n",
    "print(str(arr_to_tensor) + \"\\n\" * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh would you look at that! PyTorch automatically picked up one the fact that the numpy array was a short and automatically cast itself. This is a double edged sword since most of the time you would want to use a float instead of an int. To remedy this you can just specify the dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "arr_to_tensor = torch.tensor(np_arr, dtype=torch.float64) # Let's use float64 to show this off\n",
    "print(str(arr_to_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "Okay types are cool and all, but what about operations? Well they're pretty much the same as numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 2  4  6  8]\n",
      " [ 3  6  9 12]\n",
      " [ 4  8 12 16]]\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 2.,  4.,  6.,  8.],\n",
      "        [ 3.,  6.,  9., 12.],\n",
      "        [ 4.,  8., 12., 16.]])\n",
      "====================\n",
      "2.5\n",
      "tensor(2.5000)\n",
      "====================\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "====================\n",
      "[1 2 3 4]\n",
      "tensor([1., 2., 3., 4.])\n",
      "====================\n",
      "[1]\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "example_np = np.array([[1, 2, 3, 4]]) # The classic\n",
    "example_torch = torch.Tensor(example_np) # Note that PyTorch will treat this like a 2D matrix since it's not specified as a vector!\n",
    "\n",
    "# Lets do a dot\n",
    "print(example_np.T @ example_np)\n",
    "print(example_torch.T @ example_torch)\n",
    "\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Mean\n",
    "print(example_np.mean())\n",
    "print(example_torch.mean())\n",
    "\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Reshape\n",
    "example_np = example_np.reshape((2,2))\n",
    "example_torch = example_torch.reshape((2,2))\n",
    "\n",
    "print(example_np)\n",
    "print(example_torch)\n",
    "\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Flatten...\n",
    "\n",
    "print(example_np.flatten())\n",
    "print(example_torch.flatten())\n",
    "\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Index by comparison, you get the idea\n",
    "\n",
    "print(example_np[example_np == 1])\n",
    "print(example_torch[example_torch == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "Now that we understand a few of the basic operations and know what tensors are lets look at autograd. Autograd, essentially short hand for auto gradient, is PyTorch's way of calculating gradients. It does this by building a computation graph, keeping track of every operation performed. It then works backwards through the graph computing the gradient at each step. Despite the gradient being calculated at every step, it's only saved / provided for the leaves (this is done to save memory). Therefore it's important that we tell PyTorch that we want the gradients calculated and saved by specifying `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27., 48.])\n",
      "tensor([18., 48.])\n",
      "====================\n",
      "tensor([54., 96.])\n",
      "tensor([36., 96.])\n",
      "====================\n",
      "tensor([27., 48.])\n",
      "tensor([18., 48.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.], requires_grad=True) # You can require gradients during initialization...\n",
    "y = torch.tensor([3., 4.])\n",
    "y.requires_grad = True # Or you can specify it after\n",
    "\n",
    "z = 3 * x @ y ** 2 # Do calculations...\n",
    "z.backward() # Calculate the gradient...\n",
    "\n",
    "print(x.grad) # Profit!\n",
    "print(y.grad)\n",
    "print(\"=\" * 20)\n",
    "\n",
    "z = 3 * x @ y ** 2 # Again!\n",
    "z.backward() # This ended up adding the gradients?\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(\"=\" * 20)\n",
    "\n",
    "x.grad.zero_() # Need to clear em\n",
    "y.grad.zero_()\n",
    "\n",
    "z = 3 * x @ y ** 2\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Now let's move on to some more advance stuff, models. Models are comprised of Layers and are a very nice abstraction that makes life a lot easier. However, these abstractions remove you from the core math and won't utilized for many, if not all, of our assignments. In order to make use of custom models we need to inheret from the `torch.nn.Module` class. Lets look at an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "\n",
    "    # Init our class\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(LogisticRegression, self).__init__() # Initialize nn.Module\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(in_dim, 20) # Just a simple linear layer\n",
    "        self.linear2 = torch.nn.Linear(20, out_dim)\n",
    "\n",
    "    # This is used for forwards propagation. The `x` given is the data that's run through the model\n",
    "    # Note that we don't need to specifiy backwards, PyTorch will take care of that.\n",
    "    def forward(self, x):\n",
    "        z1 = self.linear1(x) # Multiply our weights with our data\n",
    "        r1 = torch.relu(z1)\n",
    "        z2 = self.linear2(r1)\n",
    "        return torch.sigmoid(z2) # Use sigmoid as our activation function and return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. That's our entire neural network defined in one easy go. We have a single layer and our activation function. We could add more, but for now let's keep it simple. For reference and ease below are a few functions that we've used before in our class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flower_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m, D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m, 1), dtype = 'int8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "    \n",
    "    X = X\n",
    "    Y = Y.ravel()\n",
    "\n",
    "    return torch.Tensor(X), torch.Tensor(Y), 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion and Optimizer\n",
    "\n",
    "To use our network we need to use a criterion, in our case Binary Cross Entropy Loss, and an optimizer, Standard Gradient Descent. The criterion is used to calculate the loss and the optimizer is used to 'optimize' our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(2, 1) # Initalize our model, 2 in, 1 out\n",
    "\n",
    "criterion = torch.nn.BCELoss() # Just initalize our loss function and we're good to go\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) # The optimizer needs the parameters from the model to know what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.75%, Loss: 0.7287843227386475\n",
      "Accuracy: 53.0%, Loss: 0.6476933360099792\n",
      "Accuracy: 55.0%, Loss: 0.6202825307846069\n",
      "Accuracy: 77.25%, Loss: 0.5901021361351013\n",
      "Accuracy: 82.5%, Loss: 0.5546938180923462\n",
      "Accuracy: 83.75%, Loss: 0.5162571668624878\n",
      "Accuracy: 84.25%, Loss: 0.47967174649238586\n",
      "Accuracy: 85.25%, Loss: 0.4481479525566101\n",
      "Accuracy: 85.0%, Loss: 0.42337632179260254\n",
      "Accuracy: 85.25%, Loss: 0.40423640608787537\n",
      "Accuracy: 85.25%, Loss: 0.38952815532684326\n",
      "Accuracy: 85.25%, Loss: 0.377664715051651\n",
      "Accuracy: 86.0%, Loss: 0.368179589509964\n",
      "Accuracy: 86.0%, Loss: 0.3605550527572632\n",
      "Accuracy: 86.25%, Loss: 0.35429054498672485\n",
      "Accuracy: 86.5%, Loss: 0.34880149364471436\n",
      "Accuracy: 86.25%, Loss: 0.3443659842014313\n",
      "Accuracy: 86.5%, Loss: 0.34061482548713684\n",
      "Accuracy: 86.25%, Loss: 0.3372609317302704\n",
      "Accuracy: 86.5%, Loss: 0.33418169617652893\n"
     ]
    }
   ],
   "source": [
    "X, Y, k = load_flower_dataset() # Load dataset\n",
    "\n",
    "for e in range(10000):\n",
    "    optimizer.zero_grad() # Zero the grad at each step\n",
    "    out = model(X) # Run our data through the model\n",
    "    loss = criterion(torch.squeeze(out), Y) # Get the loss\n",
    "\n",
    "    loss.backward() # Run back prop\n",
    "    optimizer.step() # Update our model\n",
    "\n",
    "    if e % 500 == 1:\n",
    "        x_a = torch.squeeze(out).round().detach().numpy() # Convert the model output to labels\n",
    "        acc = np.sum(x_a == Y.detach().numpy()) # Get the accuracy\n",
    "        print(f\"Accuracy: {acc / 4}%, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's really all there is too using PyTorch. It makes designing and implementing neural networks very easy if you understand what needs to be done. \n",
    "\n",
    "### Tensorboard\n",
    "Tensorboard was originally implemented for use with Tensorboard. It was such a good idea that [PyTorch added support for it](https://pytorch.org/docs/stable/tensorboard.html). \n",
    "\n",
    "Tensorboard is used via the Summary Writer. There you can record data for each epoch as well as using it to view the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.0%, Loss: 0.6761515140533447\n",
      "Accuracy: 54.5%, Loss: 0.6401860117912292\n",
      "Accuracy: 69.25%, Loss: 0.6141994595527649\n",
      "Accuracy: 79.5%, Loss: 0.5809131860733032\n",
      "Accuracy: 82.25%, Loss: 0.541892409324646\n",
      "Accuracy: 81.5%, Loss: 0.5038732290267944\n",
      "Accuracy: 81.5%, Loss: 0.47087135910987854\n",
      "Accuracy: 82.25%, Loss: 0.4439656138420105\n",
      "Accuracy: 83.75%, Loss: 0.4220562279224396\n",
      "Accuracy: 83.75%, Loss: 0.4041001498699188\n",
      "Accuracy: 84.0%, Loss: 0.390156626701355\n",
      "Accuracy: 84.5%, Loss: 0.37916630506515503\n",
      "Accuracy: 84.25%, Loss: 0.37023529410362244\n",
      "Accuracy: 84.5%, Loss: 0.36289364099502563\n",
      "Accuracy: 85.0%, Loss: 0.3566746115684509\n",
      "Accuracy: 84.75%, Loss: 0.35133111476898193\n",
      "Accuracy: 84.75%, Loss: 0.346628874540329\n",
      "Accuracy: 85.0%, Loss: 0.3424755036830902\n",
      "Accuracy: 85.0%, Loss: 0.33900272846221924\n",
      "Accuracy: 85.0%, Loss: 0.3359600007534027\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter # This is how we import it\n",
    "\n",
    "writer = SummaryWriter() # Simple init, we will write stuff to it below...\n",
    "\n",
    "model = LogisticRegression(2, 1) # Initalize our model, 2 in, 1 out\n",
    "\n",
    "criterion = torch.nn.BCELoss() # Just initalize our loss function and we're good to go\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) # The optimizer needs the parameters from the model to know what to do\n",
    "\n",
    "X, Y, k = load_flower_dataset() # Load dataset\n",
    "\n",
    "\n",
    "writer.add_graph(model, X) # Get the graph for the model\n",
    "\n",
    "for e in range(10000):\n",
    "    optimizer.zero_grad() # Zero the grad at each step\n",
    "    out = model(X) # Run our data through the model\n",
    "    loss = criterion(torch.squeeze(out), Y) # Get the loss\n",
    "\n",
    "    loss.backward() # Run back prop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss, e) # Record the loss at each step....\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.step() # Update our model\n",
    "\n",
    "    if e % 500 == 1:\n",
    "        x_a = torch.squeeze(out).round().detach().numpy() # Convert the model output to labels\n",
    "        acc = np.sum(x_a == Y.detach().numpy()) # Get the accuracy\n",
    "\n",
    "        writer.add_scalar(\"Loss/accuracy\", acc, e) # Record the accuracy\n",
    "        \n",
    "        print(f\"Accuracy: {acc / 4}%, Loss: {loss}\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
